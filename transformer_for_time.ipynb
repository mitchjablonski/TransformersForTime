{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"Calculate the attention weights.\n",
    "    q, k, v must have matching leading dimensions.\n",
    "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "    The mask has different shapes depending on its type(padding or look ahead) \n",
    "    but it must be broadcastable for addition.\n",
    "    \n",
    "    Args:\n",
    "      q: query shape == (..., seq_len_q, depth)\n",
    "      k: key shape == (..., seq_len_k, depth)\n",
    "      v: value shape == (..., seq_len_v, depth_v)\n",
    "      mask: Float tensor with shape broadcastable \n",
    "            to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "      \n",
    "    Returns:\n",
    "      output, attention_weights\n",
    "    \"\"\"\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "    \n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)  \n",
    " \n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    " \n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    " \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads,\n",
    "                 q: int,\n",
    "                 v: int,\n",
    "                 h: int):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "    \n",
    "    assert d_model % self.num_heads == 0\n",
    "    \n",
    "    self.depth = d_model // self.num_heads\n",
    "    \n",
    "    self.wq = tf.keras.layers.Dense(q*h)\n",
    "    self.wk = tf.keras.layers.Dense(q*h)\n",
    "    self.wv = tf.keras.layers.Dense(v*h)\n",
    "    \n",
    "    self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "  def split_heads(self, x, batch_size):\n",
    "    \"\"\"Split the last dimension into (num_heads, depth).\n",
    "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "    \"\"\"\n",
    "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "  def call(self, v, k, q, mask):\n",
    "    batch_size = tf.shape(q)[0]\n",
    "    \n",
    "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "    \n",
    "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "    \n",
    "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "        q, k, v, mask)\n",
    "    \n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "    concat_attention = tf.reshape(scaled_attention, \n",
    "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "        \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, \n",
    "                 q: int,\n",
    "                 v: int,\n",
    "                 h: int,\n",
    "               rate=0.1):\n",
    "    super(EncoderLayer, self).__init__()\n",
    "\n",
    "    self.mha = MultiHeadAttention(d_model, num_heads, q, v, h)\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "  def call(self, x, training, mask):\n",
    "\n",
    "    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "    attn_output = self.dropout1(attn_output, training=training)\n",
    "    print('enc shpaes', x.shape, attn_output.shape)\n",
    "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "    \n",
    "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "    ffn_output = self.dropout2(ffn_output, training=training)\n",
    "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "    \n",
    "    return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "  return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, \n",
    "                 q: int,\n",
    "                 v: int,\n",
    "                 h: int,\n",
    "                 rate=0.1):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "\n",
    "    self.mha1 = MultiHeadAttention(d_model, num_heads, q, v, h)\n",
    "    self.mha2 = MultiHeadAttention(d_model, num_heads, q, v, h)\n",
    "\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    " \n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    \n",
    "  def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "    # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "    print('dec,',x.shape, 'enc_out', enc_output.shape)\n",
    "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "    attn1 = self.dropout1(attn1, training=training)\n",
    "    print('devc2,',attn1.shape, 'dec', x.shape)\n",
    "    out1 = self.layernorm1(attn1 + x)\n",
    "    \n",
    "    attn2, attn_weights_block2 = self.mha2(\n",
    "        enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "    attn2 = self.dropout2(attn2, training=training)\n",
    "    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "    \n",
    "    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "    ffn_output = self.dropout3(ffn_output, training=training)\n",
    "    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "    \n",
    "    return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to add x + attn1, attn1 must have shape of input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "  def __init__(self,\n",
    "                 #d_input: int,\n",
    "                 d_model: int,\n",
    "                 d_output: int,\n",
    "                 num_heads: int,\n",
    "                 num_layers: int,\n",
    "                 q: int,\n",
    "                 v: int,\n",
    "                 h: int,\n",
    "                 #attention_size: int = None,\n",
    "                 dropout: float = 0.3,\n",
    "                 dff: int = 2048,\n",
    "                 pe: str = None):\n",
    "    super(Transformer, self).__init__()\n",
    "\n",
    "    self.encoder = Encoder(d_model, num_heads, num_layers,dff,q, v, h,\n",
    "                            dropout=dropout)#attention_size=attention_size,\n",
    "\n",
    "    self.decoder = Decoder(d_model, num_heads, num_layers,dff, q, v, h,\n",
    "                            dropout=dropout)#attention_size=attention_size,\n",
    "\n",
    "    self.inp_embed = tf.keras.layers.Dense(d_model)\n",
    "    self.tar_embed = tf.keras.layers.Dense(d_model)\n",
    "    self.dec_embed = tf.keras.layers.Dense(d_model)\n",
    "    self.final_layer = tf.keras.layers.Dense(d_output)\n",
    "    \n",
    "  def call(self, inp, tar, training, enc_padding_mask, \n",
    "           look_ahead_mask, dec_padding_mask): #tar\n",
    "    \n",
    "    print(inp.shape)\n",
    "    inp_reform = self.inp_embed(inp) #nn.Linear(d_input, d_model)\n",
    "    print(inp_reform.shape)\n",
    "    enc_output = self.encoder(inp_reform, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "    print(enc_output.shape)\n",
    "    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "    \n",
    "    print(tar.shape)\n",
    "    print(self.tar_embed)\n",
    "    tar_reform = self.tar_embed(tar)\n",
    "    decoding = enc_output \n",
    "    dec_output, attention_weights = self.decoder(\n",
    "        decoding, enc_output, training, look_ahead_mask, dec_padding_mask)#inp_reform\n",
    "    \n",
    "    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "    print('final outshape', final_output.shape)\n",
    "    return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "  def __init__(self,\n",
    "                 d_model: int,\n",
    "                 num_heads: int,\n",
    "                 num_layers: int,\n",
    "                 dff: int,\n",
    "                 q: int,\n",
    "                 v: int,\n",
    "                 h: int,\n",
    "                 dropout: float = 0.3):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "    \n",
    "    #self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "    #self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "    \n",
    "    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, q, v, h,dropout) \n",
    "                       for _ in range(num_layers)]\n",
    "    self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "    \n",
    "  def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    attention_weights = {}\n",
    "    \n",
    "    #x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "    #x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    #x += self.pos_encoding[:, :seq_len, :]\n",
    "    \n",
    "    x = self.dropout(x, training=training)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                             look_ahead_mask, padding_mask)\n",
    "      \n",
    "      attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
    "      attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
    "    \n",
    "    # x.shape == (batch_size, target_seq_len, d_model)\n",
    "    return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "  def __init__(self,\n",
    "                 d_model: int,\n",
    "                 num_heads: int,\n",
    "                 num_layers: int,\n",
    "                 dff: int,\n",
    "                 q: int,\n",
    "                 v: int,\n",
    "                 h: int,\n",
    "                 dropout: float = 0.3):\n",
    "    super(Encoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "    \n",
    "    #self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "    #self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "    #                                        self.d_model)\n",
    "    \n",
    "    \n",
    "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff,q,v,h, dropout) \n",
    "                       for _ in range(num_layers)]\n",
    "  \n",
    "    self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "  def call(self, x, training, mask):\n",
    "\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    \n",
    "    # adding embedding and position encoding.\n",
    "    #x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "    #x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    #x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "    x = self.dropout(x, training=training)\n",
    "    \n",
    "    for i in range(self.num_layers):\n",
    "      x = self.enc_layers[i](x, training, mask)\n",
    "    \n",
    "    return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, timesteps, input_dim = None, 20, 30\n",
    "out_time=1\n",
    "out_dim = 8\n",
    "def get_x_y(size=1000):\n",
    "    import numpy as np\n",
    "    pos_indices = np.random.choice(size, size=int(size // 2), replace=False)\n",
    "    x_train = np.zeros(shape=(size, timesteps, input_dim))\n",
    "    y_train = np.zeros(shape=(size, out_time, out_dim))\n",
    "    x_train[pos_indices, 0] = 1.0\n",
    "    y_train[pos_indices, 0] = 1.0\n",
    "    return x_train, y_train\n",
    "x, y = get_x_y()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BATCH_SIZE = 8\n",
    "#NUM_WORKERS = 0\n",
    "LR = 2e-4\n",
    "EPOCHS = 30\n",
    "q=v=h = 8\n",
    "# Model parameters\n",
    "d_model = 64 # Lattent dim\n",
    "num_heads = 2 # Number of heads\n",
    "num_layers = 4 # Number of encoder and decoder to stack\n",
    "#attention_size = 12 # Attention window size\n",
    "dropout = 0.2 # Dropout rate\n",
    "pe = None # Positional encoding\n",
    "chunk_mode = None\n",
    "\n",
    "d_input = input_dim # From dataset\n",
    "d_output = out_dim # From dataset#\n",
    "net = Transformer( d_model, d_output, num_heads, num_layers, q, v, h, dropout=dropout, pe=pe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 20, 30)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1, 8)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1012 19:51:46.693113 140211769378624 base_layer.py:2081] Layer transformer_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "W1012 19:51:46.818227 140211769378624 base_layer.py:2081] Layer dense_201 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 20, 30)\n",
      "(1000, 20, 64)\n",
      "enc shpaes (1000, 20, 64) (1000, 20, 64)\n",
      "enc shpaes (1000, 20, 64) (1000, 20, 64)\n",
      "enc shpaes (1000, 20, 64) (1000, 20, 64)\n",
      "enc shpaes (1000, 20, 64) (1000, 20, 64)\n",
      "(1000, 20, 64)\n",
      "(1000, 1, 8)\n",
      "<tensorflow.python.keras.layers.core.Dense object at 0x7f84fb3d67f0>\n",
      "dec, (1000, 20, 64) enc_out (1000, 20, 64)\n",
      "devc2, (1000, 20, 64) dec (1000, 20, 64)\n",
      "dec, (1000, 20, 64) enc_out (1000, 20, 64)\n",
      "devc2, (1000, 20, 64) dec (1000, 20, 64)\n",
      "dec, (1000, 20, 64) enc_out (1000, 20, 64)\n",
      "devc2, (1000, 20, 64) dec (1000, 20, 64)\n",
      "dec, (1000, 20, 64) enc_out (1000, 20, 64)\n",
      "devc2, (1000, 20, 64) dec (1000, 20, 64)\n",
      "final outshape (1000, 20, 8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1000, 20, 8), dtype=float32, numpy=\n",
       " array([[[ 0.06893065,  1.7473276 ,  2.1983192 , ...,  0.48462397,\n",
       "          -2.7826164 ,  0.9646784 ],\n",
       "         [ 0.02563114,  1.8055763 ,  2.2126632 , ...,  0.53761905,\n",
       "          -2.9179316 ,  0.9723504 ],\n",
       "         [ 0.02563114,  1.8055763 ,  2.2126632 , ...,  0.53761905,\n",
       "          -2.9179316 ,  0.9723504 ],\n",
       "         ...,\n",
       "         [ 0.02563114,  1.8055763 ,  2.2126632 , ...,  0.53761905,\n",
       "          -2.9179316 ,  0.9723504 ],\n",
       "         [ 0.02563114,  1.8055763 ,  2.2126632 , ...,  0.53761905,\n",
       "          -2.9179316 ,  0.9723504 ],\n",
       "         [ 0.02563114,  1.8055763 ,  2.2126632 , ...,  0.53761905,\n",
       "          -2.9179316 ,  0.9723504 ]],\n",
       " \n",
       "        [[ 0.06893065,  1.7473276 ,  2.1983192 , ...,  0.48462397,\n",
       "          -2.7826164 ,  0.9646784 ],\n",
       "         [ 0.02563114,  1.8055763 ,  2.2126632 , ...,  0.53761905,\n",
       "          -2.9179316 ,  0.9723504 ],\n",
       "         [ 0.02563114,  1.8055763 ,  2.2126632 , ...,  0.53761905,\n",
       "          -2.9179316 ,  0.9723504 ],\n",
       "         ...,\n",
       "         [ 0.02563114,  1.8055763 ,  2.2126632 , ...,  0.53761905,\n",
       "          -2.9179316 ,  0.9723504 ],\n",
       "         [ 0.02563114,  1.8055763 ,  2.2126632 , ...,  0.53761905,\n",
       "          -2.9179316 ,  0.9723504 ],\n",
       "         [ 0.02563114,  1.8055763 ,  2.2126632 , ...,  0.53761905,\n",
       "          -2.9179316 ,  0.9723504 ]],\n",
       " \n",
       "        [[ 0.06893065,  1.7473276 ,  2.1983192 , ...,  0.48462397,\n",
       "          -2.7826164 ,  0.9646784 ],\n",
       "         [ 0.02563114,  1.8055763 ,  2.2126632 , ...,  0.53761905,\n",
       "          -2.9179316 ,  0.9723504 ],\n",
       "         [ 0.02563114,  1.8055763 ,  2.2126632 , ...,  0.53761905,\n",
       "          -2.9179316 ,  0.9723504 ],\n",
       "         ...,\n",
       "         [ 0.02563114,  1.8055763 ,  2.2126632 , ...,  0.53761905,\n",
       "          -2.9179316 ,  0.9723504 ],\n",
       "         [ 0.02563114,  1.8055763 ,  2.2126632 , ...,  0.53761905,\n",
       "          -2.9179316 ,  0.9723504 ],\n",
       "         [ 0.02563114,  1.8055763 ,  2.2126632 , ...,  0.53761905,\n",
       "          -2.9179316 ,  0.9723504 ]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 0.06893065,  1.7473276 ,  2.1983192 , ...,  0.48462397,\n",
       "          -2.7826164 ,  0.9646784 ],\n",
       "         [ 0.02563114,  1.8055763 ,  2.2126632 , ...,  0.53761905,\n",
       "          -2.9179316 ,  0.9723504 ],\n",
       "         [ 0.02563114,  1.8055763 ,  2.2126632 , ...,  0.53761905,\n",
       "          -2.9179316 ,  0.9723504 ],\n",
       "         ...,\n",
       "         [ 0.02563114,  1.8055763 ,  2.2126632 , ...,  0.53761905,\n",
       "          -2.9179316 ,  0.9723504 ],\n",
       "         [ 0.02563114,  1.8055763 ,  2.2126632 , ...,  0.53761905,\n",
       "          -2.9179316 ,  0.9723504 ],\n",
       "         [ 0.02563114,  1.8055763 ,  2.2126632 , ...,  0.53761905,\n",
       "          -2.9179316 ,  0.9723504 ]],\n",
       " \n",
       "        [[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         ...,\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ]],\n",
       " \n",
       "        [[ 0.06893065,  1.7473276 ,  2.1983192 , ...,  0.48462397,\n",
       "          -2.7826164 ,  0.9646784 ],\n",
       "         [ 0.02563114,  1.8055763 ,  2.2126632 , ...,  0.53761905,\n",
       "          -2.9179316 ,  0.9723504 ],\n",
       "         [ 0.02563114,  1.8055763 ,  2.2126632 , ...,  0.53761905,\n",
       "          -2.9179316 ,  0.9723504 ],\n",
       "         ...,\n",
       "         [ 0.02563114,  1.8055763 ,  2.2126632 , ...,  0.53761905,\n",
       "          -2.9179316 ,  0.9723504 ],\n",
       "         [ 0.02563114,  1.8055763 ,  2.2126632 , ...,  0.53761905,\n",
       "          -2.9179316 ,  0.9723504 ],\n",
       "         [ 0.02563114,  1.8055763 ,  2.2126632 , ...,  0.53761905,\n",
       "          -2.9179316 ,  0.9723504 ]]], dtype=float32)>,\n",
       " {'decoder_layer1_block1': <tf.Tensor: shape=(1000, 2, 20, 20), dtype=float32, numpy=\n",
       "  array([[[[0.08415227, 0.04820251, 0.04820251, ..., 0.04820251,\n",
       "            0.04820251, 0.04820251],\n",
       "           [0.03659664, 0.05070544, 0.05070544, ..., 0.05070544,\n",
       "            0.05070544, 0.05070544],\n",
       "           [0.03659664, 0.05070544, 0.05070544, ..., 0.05070544,\n",
       "            0.05070544, 0.05070544],\n",
       "           ...,\n",
       "           [0.03659664, 0.05070544, 0.05070544, ..., 0.05070544,\n",
       "            0.05070544, 0.05070544],\n",
       "           [0.03659664, 0.05070544, 0.05070544, ..., 0.05070544,\n",
       "            0.05070544, 0.05070544],\n",
       "           [0.03659664, 0.05070544, 0.05070544, ..., 0.05070544,\n",
       "            0.05070544, 0.05070544]],\n",
       "  \n",
       "          [[0.04382319, 0.0503251 , 0.0503251 , ..., 0.0503251 ,\n",
       "            0.0503251 , 0.0503251 ],\n",
       "           [0.05255757, 0.04986539, 0.04986539, ..., 0.04986539,\n",
       "            0.04986539, 0.04986539],\n",
       "           [0.05255757, 0.04986539, 0.04986539, ..., 0.04986539,\n",
       "            0.04986539, 0.04986539],\n",
       "           ...,\n",
       "           [0.05255757, 0.04986539, 0.04986539, ..., 0.04986539,\n",
       "            0.04986539, 0.04986539],\n",
       "           [0.05255757, 0.04986539, 0.04986539, ..., 0.04986539,\n",
       "            0.04986539, 0.04986539],\n",
       "           [0.05255757, 0.04986539, 0.04986539, ..., 0.04986539,\n",
       "            0.04986539, 0.04986539]]],\n",
       "  \n",
       "  \n",
       "         [[[0.08415227, 0.04820251, 0.04820251, ..., 0.04820251,\n",
       "            0.04820251, 0.04820251],\n",
       "           [0.03659664, 0.05070544, 0.05070544, ..., 0.05070544,\n",
       "            0.05070544, 0.05070544],\n",
       "           [0.03659664, 0.05070544, 0.05070544, ..., 0.05070544,\n",
       "            0.05070544, 0.05070544],\n",
       "           ...,\n",
       "           [0.03659664, 0.05070544, 0.05070544, ..., 0.05070544,\n",
       "            0.05070544, 0.05070544],\n",
       "           [0.03659664, 0.05070544, 0.05070544, ..., 0.05070544,\n",
       "            0.05070544, 0.05070544],\n",
       "           [0.03659664, 0.05070544, 0.05070544, ..., 0.05070544,\n",
       "            0.05070544, 0.05070544]],\n",
       "  \n",
       "          [[0.04382319, 0.0503251 , 0.0503251 , ..., 0.0503251 ,\n",
       "            0.0503251 , 0.0503251 ],\n",
       "           [0.05255757, 0.04986539, 0.04986539, ..., 0.04986539,\n",
       "            0.04986539, 0.04986539],\n",
       "           [0.05255757, 0.04986539, 0.04986539, ..., 0.04986539,\n",
       "            0.04986539, 0.04986539],\n",
       "           ...,\n",
       "           [0.05255757, 0.04986539, 0.04986539, ..., 0.04986539,\n",
       "            0.04986539, 0.04986539],\n",
       "           [0.05255757, 0.04986539, 0.04986539, ..., 0.04986539,\n",
       "            0.04986539, 0.04986539],\n",
       "           [0.05255757, 0.04986539, 0.04986539, ..., 0.04986539,\n",
       "            0.04986539, 0.04986539]]],\n",
       "  \n",
       "  \n",
       "         [[[0.08415227, 0.04820251, 0.04820251, ..., 0.04820251,\n",
       "            0.04820251, 0.04820251],\n",
       "           [0.03659664, 0.05070544, 0.05070544, ..., 0.05070544,\n",
       "            0.05070544, 0.05070544],\n",
       "           [0.03659664, 0.05070544, 0.05070544, ..., 0.05070544,\n",
       "            0.05070544, 0.05070544],\n",
       "           ...,\n",
       "           [0.03659664, 0.05070544, 0.05070544, ..., 0.05070544,\n",
       "            0.05070544, 0.05070544],\n",
       "           [0.03659664, 0.05070544, 0.05070544, ..., 0.05070544,\n",
       "            0.05070544, 0.05070544],\n",
       "           [0.03659664, 0.05070544, 0.05070544, ..., 0.05070544,\n",
       "            0.05070544, 0.05070544]],\n",
       "  \n",
       "          [[0.04382319, 0.0503251 , 0.0503251 , ..., 0.0503251 ,\n",
       "            0.0503251 , 0.0503251 ],\n",
       "           [0.05255757, 0.04986539, 0.04986539, ..., 0.04986539,\n",
       "            0.04986539, 0.04986539],\n",
       "           [0.05255757, 0.04986539, 0.04986539, ..., 0.04986539,\n",
       "            0.04986539, 0.04986539],\n",
       "           ...,\n",
       "           [0.05255757, 0.04986539, 0.04986539, ..., 0.04986539,\n",
       "            0.04986539, 0.04986539],\n",
       "           [0.05255757, 0.04986539, 0.04986539, ..., 0.04986539,\n",
       "            0.04986539, 0.04986539],\n",
       "           [0.05255757, 0.04986539, 0.04986539, ..., 0.04986539,\n",
       "            0.04986539, 0.04986539]]],\n",
       "  \n",
       "  \n",
       "         ...,\n",
       "  \n",
       "  \n",
       "         [[[0.08415227, 0.04820251, 0.04820251, ..., 0.04820251,\n",
       "            0.04820251, 0.04820251],\n",
       "           [0.03659664, 0.05070544, 0.05070544, ..., 0.05070544,\n",
       "            0.05070544, 0.05070544],\n",
       "           [0.03659664, 0.05070544, 0.05070544, ..., 0.05070544,\n",
       "            0.05070544, 0.05070544],\n",
       "           ...,\n",
       "           [0.03659664, 0.05070544, 0.05070544, ..., 0.05070544,\n",
       "            0.05070544, 0.05070544],\n",
       "           [0.03659664, 0.05070544, 0.05070544, ..., 0.05070544,\n",
       "            0.05070544, 0.05070544],\n",
       "           [0.03659664, 0.05070544, 0.05070544, ..., 0.05070544,\n",
       "            0.05070544, 0.05070544]],\n",
       "  \n",
       "          [[0.04382319, 0.0503251 , 0.0503251 , ..., 0.0503251 ,\n",
       "            0.0503251 , 0.0503251 ],\n",
       "           [0.05255757, 0.04986539, 0.04986539, ..., 0.04986539,\n",
       "            0.04986539, 0.04986539],\n",
       "           [0.05255757, 0.04986539, 0.04986539, ..., 0.04986539,\n",
       "            0.04986539, 0.04986539],\n",
       "           ...,\n",
       "           [0.05255757, 0.04986539, 0.04986539, ..., 0.04986539,\n",
       "            0.04986539, 0.04986539],\n",
       "           [0.05255757, 0.04986539, 0.04986539, ..., 0.04986539,\n",
       "            0.04986539, 0.04986539],\n",
       "           [0.05255757, 0.04986539, 0.04986539, ..., 0.04986539,\n",
       "            0.04986539, 0.04986539]]],\n",
       "  \n",
       "  \n",
       "         [[[0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           ...,\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ]],\n",
       "  \n",
       "          [[0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           ...,\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ]]],\n",
       "  \n",
       "  \n",
       "         [[[0.08415227, 0.04820251, 0.04820251, ..., 0.04820251,\n",
       "            0.04820251, 0.04820251],\n",
       "           [0.03659664, 0.05070544, 0.05070544, ..., 0.05070544,\n",
       "            0.05070544, 0.05070544],\n",
       "           [0.03659664, 0.05070544, 0.05070544, ..., 0.05070544,\n",
       "            0.05070544, 0.05070544],\n",
       "           ...,\n",
       "           [0.03659664, 0.05070544, 0.05070544, ..., 0.05070544,\n",
       "            0.05070544, 0.05070544],\n",
       "           [0.03659664, 0.05070544, 0.05070544, ..., 0.05070544,\n",
       "            0.05070544, 0.05070544],\n",
       "           [0.03659664, 0.05070544, 0.05070544, ..., 0.05070544,\n",
       "            0.05070544, 0.05070544]],\n",
       "  \n",
       "          [[0.04382319, 0.0503251 , 0.0503251 , ..., 0.0503251 ,\n",
       "            0.0503251 , 0.0503251 ],\n",
       "           [0.05255757, 0.04986539, 0.04986539, ..., 0.04986539,\n",
       "            0.04986539, 0.04986539],\n",
       "           [0.05255757, 0.04986539, 0.04986539, ..., 0.04986539,\n",
       "            0.04986539, 0.04986539],\n",
       "           ...,\n",
       "           [0.05255757, 0.04986539, 0.04986539, ..., 0.04986539,\n",
       "            0.04986539, 0.04986539],\n",
       "           [0.05255757, 0.04986539, 0.04986539, ..., 0.04986539,\n",
       "            0.04986539, 0.04986539],\n",
       "           [0.05255757, 0.04986539, 0.04986539, ..., 0.04986539,\n",
       "            0.04986539, 0.04986539]]]], dtype=float32)>,\n",
       "  'decoder_layer1_block2': <tf.Tensor: shape=(1000, 2, 20, 20), dtype=float32, numpy=\n",
       "  array([[[[0.04085441, 0.05048135, 0.05048135, ..., 0.05048135,\n",
       "            0.05048135, 0.05048135],\n",
       "           [0.05111579, 0.04994127, 0.04994127, ..., 0.04994127,\n",
       "            0.04994127, 0.04994127],\n",
       "           [0.05111579, 0.04994127, 0.04994127, ..., 0.04994127,\n",
       "            0.04994127, 0.04994127],\n",
       "           ...,\n",
       "           [0.05111579, 0.04994127, 0.04994127, ..., 0.04994127,\n",
       "            0.04994127, 0.04994127],\n",
       "           [0.05111579, 0.04994127, 0.04994127, ..., 0.04994127,\n",
       "            0.04994127, 0.04994127],\n",
       "           [0.05111579, 0.04994127, 0.04994127, ..., 0.04994127,\n",
       "            0.04994127, 0.04994127]],\n",
       "  \n",
       "          [[0.04650681, 0.05018385, 0.05018385, ..., 0.05018385,\n",
       "            0.05018385, 0.05018385],\n",
       "           [0.03386215, 0.05084936, 0.05084936, ..., 0.05084936,\n",
       "            0.05084936, 0.05084936],\n",
       "           [0.03386215, 0.05084936, 0.05084936, ..., 0.05084936,\n",
       "            0.05084936, 0.05084936],\n",
       "           ...,\n",
       "           [0.03386215, 0.05084936, 0.05084936, ..., 0.05084936,\n",
       "            0.05084936, 0.05084936],\n",
       "           [0.03386215, 0.05084936, 0.05084936, ..., 0.05084936,\n",
       "            0.05084936, 0.05084936],\n",
       "           [0.03386215, 0.05084936, 0.05084936, ..., 0.05084936,\n",
       "            0.05084936, 0.05084936]]],\n",
       "  \n",
       "  \n",
       "         [[[0.04085441, 0.05048135, 0.05048135, ..., 0.05048135,\n",
       "            0.05048135, 0.05048135],\n",
       "           [0.05111579, 0.04994127, 0.04994127, ..., 0.04994127,\n",
       "            0.04994127, 0.04994127],\n",
       "           [0.05111579, 0.04994127, 0.04994127, ..., 0.04994127,\n",
       "            0.04994127, 0.04994127],\n",
       "           ...,\n",
       "           [0.05111579, 0.04994127, 0.04994127, ..., 0.04994127,\n",
       "            0.04994127, 0.04994127],\n",
       "           [0.05111579, 0.04994127, 0.04994127, ..., 0.04994127,\n",
       "            0.04994127, 0.04994127],\n",
       "           [0.05111579, 0.04994127, 0.04994127, ..., 0.04994127,\n",
       "            0.04994127, 0.04994127]],\n",
       "  \n",
       "          [[0.04650681, 0.05018385, 0.05018385, ..., 0.05018385,\n",
       "            0.05018385, 0.05018385],\n",
       "           [0.03386215, 0.05084936, 0.05084936, ..., 0.05084936,\n",
       "            0.05084936, 0.05084936],\n",
       "           [0.03386215, 0.05084936, 0.05084936, ..., 0.05084936,\n",
       "            0.05084936, 0.05084936],\n",
       "           ...,\n",
       "           [0.03386215, 0.05084936, 0.05084936, ..., 0.05084936,\n",
       "            0.05084936, 0.05084936],\n",
       "           [0.03386215, 0.05084936, 0.05084936, ..., 0.05084936,\n",
       "            0.05084936, 0.05084936],\n",
       "           [0.03386215, 0.05084936, 0.05084936, ..., 0.05084936,\n",
       "            0.05084936, 0.05084936]]],\n",
       "  \n",
       "  \n",
       "         [[[0.04085441, 0.05048135, 0.05048135, ..., 0.05048135,\n",
       "            0.05048135, 0.05048135],\n",
       "           [0.05111579, 0.04994127, 0.04994127, ..., 0.04994127,\n",
       "            0.04994127, 0.04994127],\n",
       "           [0.05111579, 0.04994127, 0.04994127, ..., 0.04994127,\n",
       "            0.04994127, 0.04994127],\n",
       "           ...,\n",
       "           [0.05111579, 0.04994127, 0.04994127, ..., 0.04994127,\n",
       "            0.04994127, 0.04994127],\n",
       "           [0.05111579, 0.04994127, 0.04994127, ..., 0.04994127,\n",
       "            0.04994127, 0.04994127],\n",
       "           [0.05111579, 0.04994127, 0.04994127, ..., 0.04994127,\n",
       "            0.04994127, 0.04994127]],\n",
       "  \n",
       "          [[0.04650681, 0.05018385, 0.05018385, ..., 0.05018385,\n",
       "            0.05018385, 0.05018385],\n",
       "           [0.03386215, 0.05084936, 0.05084936, ..., 0.05084936,\n",
       "            0.05084936, 0.05084936],\n",
       "           [0.03386215, 0.05084936, 0.05084936, ..., 0.05084936,\n",
       "            0.05084936, 0.05084936],\n",
       "           ...,\n",
       "           [0.03386215, 0.05084936, 0.05084936, ..., 0.05084936,\n",
       "            0.05084936, 0.05084936],\n",
       "           [0.03386215, 0.05084936, 0.05084936, ..., 0.05084936,\n",
       "            0.05084936, 0.05084936],\n",
       "           [0.03386215, 0.05084936, 0.05084936, ..., 0.05084936,\n",
       "            0.05084936, 0.05084936]]],\n",
       "  \n",
       "  \n",
       "         ...,\n",
       "  \n",
       "  \n",
       "         [[[0.04085441, 0.05048135, 0.05048135, ..., 0.05048135,\n",
       "            0.05048135, 0.05048135],\n",
       "           [0.05111579, 0.04994127, 0.04994127, ..., 0.04994127,\n",
       "            0.04994127, 0.04994127],\n",
       "           [0.05111579, 0.04994127, 0.04994127, ..., 0.04994127,\n",
       "            0.04994127, 0.04994127],\n",
       "           ...,\n",
       "           [0.05111579, 0.04994127, 0.04994127, ..., 0.04994127,\n",
       "            0.04994127, 0.04994127],\n",
       "           [0.05111579, 0.04994127, 0.04994127, ..., 0.04994127,\n",
       "            0.04994127, 0.04994127],\n",
       "           [0.05111579, 0.04994127, 0.04994127, ..., 0.04994127,\n",
       "            0.04994127, 0.04994127]],\n",
       "  \n",
       "          [[0.04650681, 0.05018385, 0.05018385, ..., 0.05018385,\n",
       "            0.05018385, 0.05018385],\n",
       "           [0.03386215, 0.05084936, 0.05084936, ..., 0.05084936,\n",
       "            0.05084936, 0.05084936],\n",
       "           [0.03386215, 0.05084936, 0.05084936, ..., 0.05084936,\n",
       "            0.05084936, 0.05084936],\n",
       "           ...,\n",
       "           [0.03386215, 0.05084936, 0.05084936, ..., 0.05084936,\n",
       "            0.05084936, 0.05084936],\n",
       "           [0.03386215, 0.05084936, 0.05084936, ..., 0.05084936,\n",
       "            0.05084936, 0.05084936],\n",
       "           [0.03386215, 0.05084936, 0.05084936, ..., 0.05084936,\n",
       "            0.05084936, 0.05084936]]],\n",
       "  \n",
       "  \n",
       "         [[[0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           ...,\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ]],\n",
       "  \n",
       "          [[0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           ...,\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ]]],\n",
       "  \n",
       "  \n",
       "         [[[0.04085441, 0.05048135, 0.05048135, ..., 0.05048135,\n",
       "            0.05048135, 0.05048135],\n",
       "           [0.05111579, 0.04994127, 0.04994127, ..., 0.04994127,\n",
       "            0.04994127, 0.04994127],\n",
       "           [0.05111579, 0.04994127, 0.04994127, ..., 0.04994127,\n",
       "            0.04994127, 0.04994127],\n",
       "           ...,\n",
       "           [0.05111579, 0.04994127, 0.04994127, ..., 0.04994127,\n",
       "            0.04994127, 0.04994127],\n",
       "           [0.05111579, 0.04994127, 0.04994127, ..., 0.04994127,\n",
       "            0.04994127, 0.04994127],\n",
       "           [0.05111579, 0.04994127, 0.04994127, ..., 0.04994127,\n",
       "            0.04994127, 0.04994127]],\n",
       "  \n",
       "          [[0.04650681, 0.05018385, 0.05018385, ..., 0.05018385,\n",
       "            0.05018385, 0.05018385],\n",
       "           [0.03386215, 0.05084936, 0.05084936, ..., 0.05084936,\n",
       "            0.05084936, 0.05084936],\n",
       "           [0.03386215, 0.05084936, 0.05084936, ..., 0.05084936,\n",
       "            0.05084936, 0.05084936],\n",
       "           ...,\n",
       "           [0.03386215, 0.05084936, 0.05084936, ..., 0.05084936,\n",
       "            0.05084936, 0.05084936],\n",
       "           [0.03386215, 0.05084936, 0.05084936, ..., 0.05084936,\n",
       "            0.05084936, 0.05084936],\n",
       "           [0.03386215, 0.05084936, 0.05084936, ..., 0.05084936,\n",
       "            0.05084936, 0.05084936]]]], dtype=float32)>,\n",
       "  'decoder_layer2_block1': <tf.Tensor: shape=(1000, 2, 20, 20), dtype=float32, numpy=\n",
       "  array([[[[0.04894347, 0.0500556 , 0.0500556 , ..., 0.0500556 ,\n",
       "            0.0500556 , 0.0500556 ],\n",
       "           [0.04780678, 0.05011543, 0.05011543, ..., 0.05011543,\n",
       "            0.05011543, 0.05011543],\n",
       "           [0.04780678, 0.05011543, 0.05011543, ..., 0.05011543,\n",
       "            0.05011543, 0.05011543],\n",
       "           ...,\n",
       "           [0.04780678, 0.05011543, 0.05011543, ..., 0.05011543,\n",
       "            0.05011543, 0.05011543],\n",
       "           [0.04780678, 0.05011543, 0.05011543, ..., 0.05011543,\n",
       "            0.05011543, 0.05011543],\n",
       "           [0.04780678, 0.05011543, 0.05011543, ..., 0.05011543,\n",
       "            0.05011543, 0.05011543]],\n",
       "  \n",
       "          [[0.06527251, 0.04919618, 0.04919618, ..., 0.04919618,\n",
       "            0.04919618, 0.04919618],\n",
       "           [0.0666622 , 0.04912304, 0.04912304, ..., 0.04912304,\n",
       "            0.04912304, 0.04912304],\n",
       "           [0.0666622 , 0.04912304, 0.04912304, ..., 0.04912304,\n",
       "            0.04912304, 0.04912304],\n",
       "           ...,\n",
       "           [0.0666622 , 0.04912304, 0.04912304, ..., 0.04912304,\n",
       "            0.04912304, 0.04912304],\n",
       "           [0.0666622 , 0.04912304, 0.04912304, ..., 0.04912304,\n",
       "            0.04912304, 0.04912304],\n",
       "           [0.0666622 , 0.04912304, 0.04912304, ..., 0.04912304,\n",
       "            0.04912304, 0.04912304]]],\n",
       "  \n",
       "  \n",
       "         [[[0.04894347, 0.0500556 , 0.0500556 , ..., 0.0500556 ,\n",
       "            0.0500556 , 0.0500556 ],\n",
       "           [0.04780678, 0.05011543, 0.05011543, ..., 0.05011543,\n",
       "            0.05011543, 0.05011543],\n",
       "           [0.04780678, 0.05011543, 0.05011543, ..., 0.05011543,\n",
       "            0.05011543, 0.05011543],\n",
       "           ...,\n",
       "           [0.04780678, 0.05011543, 0.05011543, ..., 0.05011543,\n",
       "            0.05011543, 0.05011543],\n",
       "           [0.04780678, 0.05011543, 0.05011543, ..., 0.05011543,\n",
       "            0.05011543, 0.05011543],\n",
       "           [0.04780678, 0.05011543, 0.05011543, ..., 0.05011543,\n",
       "            0.05011543, 0.05011543]],\n",
       "  \n",
       "          [[0.06527251, 0.04919618, 0.04919618, ..., 0.04919618,\n",
       "            0.04919618, 0.04919618],\n",
       "           [0.0666622 , 0.04912304, 0.04912304, ..., 0.04912304,\n",
       "            0.04912304, 0.04912304],\n",
       "           [0.0666622 , 0.04912304, 0.04912304, ..., 0.04912304,\n",
       "            0.04912304, 0.04912304],\n",
       "           ...,\n",
       "           [0.0666622 , 0.04912304, 0.04912304, ..., 0.04912304,\n",
       "            0.04912304, 0.04912304],\n",
       "           [0.0666622 , 0.04912304, 0.04912304, ..., 0.04912304,\n",
       "            0.04912304, 0.04912304],\n",
       "           [0.0666622 , 0.04912304, 0.04912304, ..., 0.04912304,\n",
       "            0.04912304, 0.04912304]]],\n",
       "  \n",
       "  \n",
       "         [[[0.04894347, 0.0500556 , 0.0500556 , ..., 0.0500556 ,\n",
       "            0.0500556 , 0.0500556 ],\n",
       "           [0.04780678, 0.05011543, 0.05011543, ..., 0.05011543,\n",
       "            0.05011543, 0.05011543],\n",
       "           [0.04780678, 0.05011543, 0.05011543, ..., 0.05011543,\n",
       "            0.05011543, 0.05011543],\n",
       "           ...,\n",
       "           [0.04780678, 0.05011543, 0.05011543, ..., 0.05011543,\n",
       "            0.05011543, 0.05011543],\n",
       "           [0.04780678, 0.05011543, 0.05011543, ..., 0.05011543,\n",
       "            0.05011543, 0.05011543],\n",
       "           [0.04780678, 0.05011543, 0.05011543, ..., 0.05011543,\n",
       "            0.05011543, 0.05011543]],\n",
       "  \n",
       "          [[0.06527251, 0.04919618, 0.04919618, ..., 0.04919618,\n",
       "            0.04919618, 0.04919618],\n",
       "           [0.0666622 , 0.04912304, 0.04912304, ..., 0.04912304,\n",
       "            0.04912304, 0.04912304],\n",
       "           [0.0666622 , 0.04912304, 0.04912304, ..., 0.04912304,\n",
       "            0.04912304, 0.04912304],\n",
       "           ...,\n",
       "           [0.0666622 , 0.04912304, 0.04912304, ..., 0.04912304,\n",
       "            0.04912304, 0.04912304],\n",
       "           [0.0666622 , 0.04912304, 0.04912304, ..., 0.04912304,\n",
       "            0.04912304, 0.04912304],\n",
       "           [0.0666622 , 0.04912304, 0.04912304, ..., 0.04912304,\n",
       "            0.04912304, 0.04912304]]],\n",
       "  \n",
       "  \n",
       "         ...,\n",
       "  \n",
       "  \n",
       "         [[[0.04894347, 0.0500556 , 0.0500556 , ..., 0.0500556 ,\n",
       "            0.0500556 , 0.0500556 ],\n",
       "           [0.04780678, 0.05011543, 0.05011543, ..., 0.05011543,\n",
       "            0.05011543, 0.05011543],\n",
       "           [0.04780678, 0.05011543, 0.05011543, ..., 0.05011543,\n",
       "            0.05011543, 0.05011543],\n",
       "           ...,\n",
       "           [0.04780678, 0.05011543, 0.05011543, ..., 0.05011543,\n",
       "            0.05011543, 0.05011543],\n",
       "           [0.04780678, 0.05011543, 0.05011543, ..., 0.05011543,\n",
       "            0.05011543, 0.05011543],\n",
       "           [0.04780678, 0.05011543, 0.05011543, ..., 0.05011543,\n",
       "            0.05011543, 0.05011543]],\n",
       "  \n",
       "          [[0.06527251, 0.04919618, 0.04919618, ..., 0.04919618,\n",
       "            0.04919618, 0.04919618],\n",
       "           [0.0666622 , 0.04912304, 0.04912304, ..., 0.04912304,\n",
       "            0.04912304, 0.04912304],\n",
       "           [0.0666622 , 0.04912304, 0.04912304, ..., 0.04912304,\n",
       "            0.04912304, 0.04912304],\n",
       "           ...,\n",
       "           [0.0666622 , 0.04912304, 0.04912304, ..., 0.04912304,\n",
       "            0.04912304, 0.04912304],\n",
       "           [0.0666622 , 0.04912304, 0.04912304, ..., 0.04912304,\n",
       "            0.04912304, 0.04912304],\n",
       "           [0.0666622 , 0.04912304, 0.04912304, ..., 0.04912304,\n",
       "            0.04912304, 0.04912304]]],\n",
       "  \n",
       "  \n",
       "         [[[0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           ...,\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ]],\n",
       "  \n",
       "          [[0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           ...,\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ]]],\n",
       "  \n",
       "  \n",
       "         [[[0.04894347, 0.0500556 , 0.0500556 , ..., 0.0500556 ,\n",
       "            0.0500556 , 0.0500556 ],\n",
       "           [0.04780678, 0.05011543, 0.05011543, ..., 0.05011543,\n",
       "            0.05011543, 0.05011543],\n",
       "           [0.04780678, 0.05011543, 0.05011543, ..., 0.05011543,\n",
       "            0.05011543, 0.05011543],\n",
       "           ...,\n",
       "           [0.04780678, 0.05011543, 0.05011543, ..., 0.05011543,\n",
       "            0.05011543, 0.05011543],\n",
       "           [0.04780678, 0.05011543, 0.05011543, ..., 0.05011543,\n",
       "            0.05011543, 0.05011543],\n",
       "           [0.04780678, 0.05011543, 0.05011543, ..., 0.05011543,\n",
       "            0.05011543, 0.05011543]],\n",
       "  \n",
       "          [[0.06527251, 0.04919618, 0.04919618, ..., 0.04919618,\n",
       "            0.04919618, 0.04919618],\n",
       "           [0.0666622 , 0.04912304, 0.04912304, ..., 0.04912304,\n",
       "            0.04912304, 0.04912304],\n",
       "           [0.0666622 , 0.04912304, 0.04912304, ..., 0.04912304,\n",
       "            0.04912304, 0.04912304],\n",
       "           ...,\n",
       "           [0.0666622 , 0.04912304, 0.04912304, ..., 0.04912304,\n",
       "            0.04912304, 0.04912304],\n",
       "           [0.0666622 , 0.04912304, 0.04912304, ..., 0.04912304,\n",
       "            0.04912304, 0.04912304],\n",
       "           [0.0666622 , 0.04912304, 0.04912304, ..., 0.04912304,\n",
       "            0.04912304, 0.04912304]]]], dtype=float32)>,\n",
       "  'decoder_layer2_block2': <tf.Tensor: shape=(1000, 2, 20, 20), dtype=float32, numpy=\n",
       "  array([[[[0.0174309 , 0.05171416, 0.05171416, ..., 0.05171416,\n",
       "            0.05171416, 0.05171416],\n",
       "           [0.01553939, 0.05181372, 0.05181372, ..., 0.05181372,\n",
       "            0.05181372, 0.05181372],\n",
       "           [0.01553939, 0.05181372, 0.05181372, ..., 0.05181372,\n",
       "            0.05181372, 0.05181372],\n",
       "           ...,\n",
       "           [0.01553939, 0.05181372, 0.05181372, ..., 0.05181372,\n",
       "            0.05181372, 0.05181372],\n",
       "           [0.01553939, 0.05181372, 0.05181372, ..., 0.05181372,\n",
       "            0.05181372, 0.05181372],\n",
       "           [0.01553939, 0.05181372, 0.05181372, ..., 0.05181372,\n",
       "            0.05181372, 0.05181372]],\n",
       "  \n",
       "          [[0.01856126, 0.05165467, 0.05165467, ..., 0.05165467,\n",
       "            0.05165467, 0.05165467],\n",
       "           [0.01718735, 0.05172698, 0.05172698, ..., 0.05172698,\n",
       "            0.05172698, 0.05172698],\n",
       "           [0.01718735, 0.05172698, 0.05172698, ..., 0.05172698,\n",
       "            0.05172698, 0.05172698],\n",
       "           ...,\n",
       "           [0.01718735, 0.05172698, 0.05172698, ..., 0.05172698,\n",
       "            0.05172698, 0.05172698],\n",
       "           [0.01718735, 0.05172698, 0.05172698, ..., 0.05172698,\n",
       "            0.05172698, 0.05172698],\n",
       "           [0.01718735, 0.05172698, 0.05172698, ..., 0.05172698,\n",
       "            0.05172698, 0.05172698]]],\n",
       "  \n",
       "  \n",
       "         [[[0.0174309 , 0.05171416, 0.05171416, ..., 0.05171416,\n",
       "            0.05171416, 0.05171416],\n",
       "           [0.01553939, 0.05181372, 0.05181372, ..., 0.05181372,\n",
       "            0.05181372, 0.05181372],\n",
       "           [0.01553939, 0.05181372, 0.05181372, ..., 0.05181372,\n",
       "            0.05181372, 0.05181372],\n",
       "           ...,\n",
       "           [0.01553939, 0.05181372, 0.05181372, ..., 0.05181372,\n",
       "            0.05181372, 0.05181372],\n",
       "           [0.01553939, 0.05181372, 0.05181372, ..., 0.05181372,\n",
       "            0.05181372, 0.05181372],\n",
       "           [0.01553939, 0.05181372, 0.05181372, ..., 0.05181372,\n",
       "            0.05181372, 0.05181372]],\n",
       "  \n",
       "          [[0.01856126, 0.05165467, 0.05165467, ..., 0.05165467,\n",
       "            0.05165467, 0.05165467],\n",
       "           [0.01718735, 0.05172698, 0.05172698, ..., 0.05172698,\n",
       "            0.05172698, 0.05172698],\n",
       "           [0.01718735, 0.05172698, 0.05172698, ..., 0.05172698,\n",
       "            0.05172698, 0.05172698],\n",
       "           ...,\n",
       "           [0.01718735, 0.05172698, 0.05172698, ..., 0.05172698,\n",
       "            0.05172698, 0.05172698],\n",
       "           [0.01718735, 0.05172698, 0.05172698, ..., 0.05172698,\n",
       "            0.05172698, 0.05172698],\n",
       "           [0.01718735, 0.05172698, 0.05172698, ..., 0.05172698,\n",
       "            0.05172698, 0.05172698]]],\n",
       "  \n",
       "  \n",
       "         [[[0.0174309 , 0.05171416, 0.05171416, ..., 0.05171416,\n",
       "            0.05171416, 0.05171416],\n",
       "           [0.01553939, 0.05181372, 0.05181372, ..., 0.05181372,\n",
       "            0.05181372, 0.05181372],\n",
       "           [0.01553939, 0.05181372, 0.05181372, ..., 0.05181372,\n",
       "            0.05181372, 0.05181372],\n",
       "           ...,\n",
       "           [0.01553939, 0.05181372, 0.05181372, ..., 0.05181372,\n",
       "            0.05181372, 0.05181372],\n",
       "           [0.01553939, 0.05181372, 0.05181372, ..., 0.05181372,\n",
       "            0.05181372, 0.05181372],\n",
       "           [0.01553939, 0.05181372, 0.05181372, ..., 0.05181372,\n",
       "            0.05181372, 0.05181372]],\n",
       "  \n",
       "          [[0.01856126, 0.05165467, 0.05165467, ..., 0.05165467,\n",
       "            0.05165467, 0.05165467],\n",
       "           [0.01718735, 0.05172698, 0.05172698, ..., 0.05172698,\n",
       "            0.05172698, 0.05172698],\n",
       "           [0.01718735, 0.05172698, 0.05172698, ..., 0.05172698,\n",
       "            0.05172698, 0.05172698],\n",
       "           ...,\n",
       "           [0.01718735, 0.05172698, 0.05172698, ..., 0.05172698,\n",
       "            0.05172698, 0.05172698],\n",
       "           [0.01718735, 0.05172698, 0.05172698, ..., 0.05172698,\n",
       "            0.05172698, 0.05172698],\n",
       "           [0.01718735, 0.05172698, 0.05172698, ..., 0.05172698,\n",
       "            0.05172698, 0.05172698]]],\n",
       "  \n",
       "  \n",
       "         ...,\n",
       "  \n",
       "  \n",
       "         [[[0.0174309 , 0.05171416, 0.05171416, ..., 0.05171416,\n",
       "            0.05171416, 0.05171416],\n",
       "           [0.01553939, 0.05181372, 0.05181372, ..., 0.05181372,\n",
       "            0.05181372, 0.05181372],\n",
       "           [0.01553939, 0.05181372, 0.05181372, ..., 0.05181372,\n",
       "            0.05181372, 0.05181372],\n",
       "           ...,\n",
       "           [0.01553939, 0.05181372, 0.05181372, ..., 0.05181372,\n",
       "            0.05181372, 0.05181372],\n",
       "           [0.01553939, 0.05181372, 0.05181372, ..., 0.05181372,\n",
       "            0.05181372, 0.05181372],\n",
       "           [0.01553939, 0.05181372, 0.05181372, ..., 0.05181372,\n",
       "            0.05181372, 0.05181372]],\n",
       "  \n",
       "          [[0.01856126, 0.05165467, 0.05165467, ..., 0.05165467,\n",
       "            0.05165467, 0.05165467],\n",
       "           [0.01718735, 0.05172698, 0.05172698, ..., 0.05172698,\n",
       "            0.05172698, 0.05172698],\n",
       "           [0.01718735, 0.05172698, 0.05172698, ..., 0.05172698,\n",
       "            0.05172698, 0.05172698],\n",
       "           ...,\n",
       "           [0.01718735, 0.05172698, 0.05172698, ..., 0.05172698,\n",
       "            0.05172698, 0.05172698],\n",
       "           [0.01718735, 0.05172698, 0.05172698, ..., 0.05172698,\n",
       "            0.05172698, 0.05172698],\n",
       "           [0.01718735, 0.05172698, 0.05172698, ..., 0.05172698,\n",
       "            0.05172698, 0.05172698]]],\n",
       "  \n",
       "  \n",
       "         [[[0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           ...,\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ]],\n",
       "  \n",
       "          [[0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           ...,\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ]]],\n",
       "  \n",
       "  \n",
       "         [[[0.0174309 , 0.05171416, 0.05171416, ..., 0.05171416,\n",
       "            0.05171416, 0.05171416],\n",
       "           [0.01553939, 0.05181372, 0.05181372, ..., 0.05181372,\n",
       "            0.05181372, 0.05181372],\n",
       "           [0.01553939, 0.05181372, 0.05181372, ..., 0.05181372,\n",
       "            0.05181372, 0.05181372],\n",
       "           ...,\n",
       "           [0.01553939, 0.05181372, 0.05181372, ..., 0.05181372,\n",
       "            0.05181372, 0.05181372],\n",
       "           [0.01553939, 0.05181372, 0.05181372, ..., 0.05181372,\n",
       "            0.05181372, 0.05181372],\n",
       "           [0.01553939, 0.05181372, 0.05181372, ..., 0.05181372,\n",
       "            0.05181372, 0.05181372]],\n",
       "  \n",
       "          [[0.01856126, 0.05165467, 0.05165467, ..., 0.05165467,\n",
       "            0.05165467, 0.05165467],\n",
       "           [0.01718735, 0.05172698, 0.05172698, ..., 0.05172698,\n",
       "            0.05172698, 0.05172698],\n",
       "           [0.01718735, 0.05172698, 0.05172698, ..., 0.05172698,\n",
       "            0.05172698, 0.05172698],\n",
       "           ...,\n",
       "           [0.01718735, 0.05172698, 0.05172698, ..., 0.05172698,\n",
       "            0.05172698, 0.05172698],\n",
       "           [0.01718735, 0.05172698, 0.05172698, ..., 0.05172698,\n",
       "            0.05172698, 0.05172698],\n",
       "           [0.01718735, 0.05172698, 0.05172698, ..., 0.05172698,\n",
       "            0.05172698, 0.05172698]]]], dtype=float32)>,\n",
       "  'decoder_layer3_block1': <tf.Tensor: shape=(1000, 2, 20, 20), dtype=float32, numpy=\n",
       "  array([[[[0.05734859, 0.04961323, 0.04961323, ..., 0.04961323,\n",
       "            0.04961323, 0.04961323],\n",
       "           [0.05340942, 0.04982056, 0.04982056, ..., 0.04982056,\n",
       "            0.04982056, 0.04982056],\n",
       "           [0.05340942, 0.04982056, 0.04982056, ..., 0.04982056,\n",
       "            0.04982056, 0.04982056],\n",
       "           ...,\n",
       "           [0.05340942, 0.04982056, 0.04982056, ..., 0.04982056,\n",
       "            0.04982056, 0.04982056],\n",
       "           [0.05340942, 0.04982056, 0.04982056, ..., 0.04982056,\n",
       "            0.04982056, 0.04982056],\n",
       "           [0.05340942, 0.04982056, 0.04982056, ..., 0.04982056,\n",
       "            0.04982056, 0.04982056]],\n",
       "  \n",
       "          [[0.0460424 , 0.0502083 , 0.0502083 , ..., 0.0502083 ,\n",
       "            0.0502083 , 0.0502083 ],\n",
       "           [0.04451874, 0.05028849, 0.05028849, ..., 0.05028849,\n",
       "            0.05028849, 0.05028849],\n",
       "           [0.04451874, 0.05028849, 0.05028849, ..., 0.05028849,\n",
       "            0.05028849, 0.05028849],\n",
       "           ...,\n",
       "           [0.04451874, 0.05028849, 0.05028849, ..., 0.05028849,\n",
       "            0.05028849, 0.05028849],\n",
       "           [0.04451874, 0.05028849, 0.05028849, ..., 0.05028849,\n",
       "            0.05028849, 0.05028849],\n",
       "           [0.04451874, 0.05028849, 0.05028849, ..., 0.05028849,\n",
       "            0.05028849, 0.05028849]]],\n",
       "  \n",
       "  \n",
       "         [[[0.05734859, 0.04961323, 0.04961323, ..., 0.04961323,\n",
       "            0.04961323, 0.04961323],\n",
       "           [0.05340942, 0.04982056, 0.04982056, ..., 0.04982056,\n",
       "            0.04982056, 0.04982056],\n",
       "           [0.05340942, 0.04982056, 0.04982056, ..., 0.04982056,\n",
       "            0.04982056, 0.04982056],\n",
       "           ...,\n",
       "           [0.05340942, 0.04982056, 0.04982056, ..., 0.04982056,\n",
       "            0.04982056, 0.04982056],\n",
       "           [0.05340942, 0.04982056, 0.04982056, ..., 0.04982056,\n",
       "            0.04982056, 0.04982056],\n",
       "           [0.05340942, 0.04982056, 0.04982056, ..., 0.04982056,\n",
       "            0.04982056, 0.04982056]],\n",
       "  \n",
       "          [[0.0460424 , 0.0502083 , 0.0502083 , ..., 0.0502083 ,\n",
       "            0.0502083 , 0.0502083 ],\n",
       "           [0.04451874, 0.05028849, 0.05028849, ..., 0.05028849,\n",
       "            0.05028849, 0.05028849],\n",
       "           [0.04451874, 0.05028849, 0.05028849, ..., 0.05028849,\n",
       "            0.05028849, 0.05028849],\n",
       "           ...,\n",
       "           [0.04451874, 0.05028849, 0.05028849, ..., 0.05028849,\n",
       "            0.05028849, 0.05028849],\n",
       "           [0.04451874, 0.05028849, 0.05028849, ..., 0.05028849,\n",
       "            0.05028849, 0.05028849],\n",
       "           [0.04451874, 0.05028849, 0.05028849, ..., 0.05028849,\n",
       "            0.05028849, 0.05028849]]],\n",
       "  \n",
       "  \n",
       "         [[[0.05734859, 0.04961323, 0.04961323, ..., 0.04961323,\n",
       "            0.04961323, 0.04961323],\n",
       "           [0.05340942, 0.04982056, 0.04982056, ..., 0.04982056,\n",
       "            0.04982056, 0.04982056],\n",
       "           [0.05340942, 0.04982056, 0.04982056, ..., 0.04982056,\n",
       "            0.04982056, 0.04982056],\n",
       "           ...,\n",
       "           [0.05340942, 0.04982056, 0.04982056, ..., 0.04982056,\n",
       "            0.04982056, 0.04982056],\n",
       "           [0.05340942, 0.04982056, 0.04982056, ..., 0.04982056,\n",
       "            0.04982056, 0.04982056],\n",
       "           [0.05340942, 0.04982056, 0.04982056, ..., 0.04982056,\n",
       "            0.04982056, 0.04982056]],\n",
       "  \n",
       "          [[0.0460424 , 0.0502083 , 0.0502083 , ..., 0.0502083 ,\n",
       "            0.0502083 , 0.0502083 ],\n",
       "           [0.04451874, 0.05028849, 0.05028849, ..., 0.05028849,\n",
       "            0.05028849, 0.05028849],\n",
       "           [0.04451874, 0.05028849, 0.05028849, ..., 0.05028849,\n",
       "            0.05028849, 0.05028849],\n",
       "           ...,\n",
       "           [0.04451874, 0.05028849, 0.05028849, ..., 0.05028849,\n",
       "            0.05028849, 0.05028849],\n",
       "           [0.04451874, 0.05028849, 0.05028849, ..., 0.05028849,\n",
       "            0.05028849, 0.05028849],\n",
       "           [0.04451874, 0.05028849, 0.05028849, ..., 0.05028849,\n",
       "            0.05028849, 0.05028849]]],\n",
       "  \n",
       "  \n",
       "         ...,\n",
       "  \n",
       "  \n",
       "         [[[0.05734859, 0.04961323, 0.04961323, ..., 0.04961323,\n",
       "            0.04961323, 0.04961323],\n",
       "           [0.05340942, 0.04982056, 0.04982056, ..., 0.04982056,\n",
       "            0.04982056, 0.04982056],\n",
       "           [0.05340942, 0.04982056, 0.04982056, ..., 0.04982056,\n",
       "            0.04982056, 0.04982056],\n",
       "           ...,\n",
       "           [0.05340942, 0.04982056, 0.04982056, ..., 0.04982056,\n",
       "            0.04982056, 0.04982056],\n",
       "           [0.05340942, 0.04982056, 0.04982056, ..., 0.04982056,\n",
       "            0.04982056, 0.04982056],\n",
       "           [0.05340942, 0.04982056, 0.04982056, ..., 0.04982056,\n",
       "            0.04982056, 0.04982056]],\n",
       "  \n",
       "          [[0.0460424 , 0.0502083 , 0.0502083 , ..., 0.0502083 ,\n",
       "            0.0502083 , 0.0502083 ],\n",
       "           [0.04451874, 0.05028849, 0.05028849, ..., 0.05028849,\n",
       "            0.05028849, 0.05028849],\n",
       "           [0.04451874, 0.05028849, 0.05028849, ..., 0.05028849,\n",
       "            0.05028849, 0.05028849],\n",
       "           ...,\n",
       "           [0.04451874, 0.05028849, 0.05028849, ..., 0.05028849,\n",
       "            0.05028849, 0.05028849],\n",
       "           [0.04451874, 0.05028849, 0.05028849, ..., 0.05028849,\n",
       "            0.05028849, 0.05028849],\n",
       "           [0.04451874, 0.05028849, 0.05028849, ..., 0.05028849,\n",
       "            0.05028849, 0.05028849]]],\n",
       "  \n",
       "  \n",
       "         [[[0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           ...,\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ]],\n",
       "  \n",
       "          [[0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           ...,\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ]]],\n",
       "  \n",
       "  \n",
       "         [[[0.05734859, 0.04961323, 0.04961323, ..., 0.04961323,\n",
       "            0.04961323, 0.04961323],\n",
       "           [0.05340942, 0.04982056, 0.04982056, ..., 0.04982056,\n",
       "            0.04982056, 0.04982056],\n",
       "           [0.05340942, 0.04982056, 0.04982056, ..., 0.04982056,\n",
       "            0.04982056, 0.04982056],\n",
       "           ...,\n",
       "           [0.05340942, 0.04982056, 0.04982056, ..., 0.04982056,\n",
       "            0.04982056, 0.04982056],\n",
       "           [0.05340942, 0.04982056, 0.04982056, ..., 0.04982056,\n",
       "            0.04982056, 0.04982056],\n",
       "           [0.05340942, 0.04982056, 0.04982056, ..., 0.04982056,\n",
       "            0.04982056, 0.04982056]],\n",
       "  \n",
       "          [[0.0460424 , 0.0502083 , 0.0502083 , ..., 0.0502083 ,\n",
       "            0.0502083 , 0.0502083 ],\n",
       "           [0.04451874, 0.05028849, 0.05028849, ..., 0.05028849,\n",
       "            0.05028849, 0.05028849],\n",
       "           [0.04451874, 0.05028849, 0.05028849, ..., 0.05028849,\n",
       "            0.05028849, 0.05028849],\n",
       "           ...,\n",
       "           [0.04451874, 0.05028849, 0.05028849, ..., 0.05028849,\n",
       "            0.05028849, 0.05028849],\n",
       "           [0.04451874, 0.05028849, 0.05028849, ..., 0.05028849,\n",
       "            0.05028849, 0.05028849],\n",
       "           [0.04451874, 0.05028849, 0.05028849, ..., 0.05028849,\n",
       "            0.05028849, 0.05028849]]]], dtype=float32)>,\n",
       "  'decoder_layer3_block2': <tf.Tensor: shape=(1000, 2, 20, 20), dtype=float32, numpy=\n",
       "  array([[[[0.09115691, 0.04783385, 0.04783385, ..., 0.04783385,\n",
       "            0.04783385, 0.04783385],\n",
       "           [0.09561975, 0.04759896, 0.04759896, ..., 0.04759896,\n",
       "            0.04759896, 0.04759896],\n",
       "           [0.09561975, 0.04759896, 0.04759896, ..., 0.04759896,\n",
       "            0.04759896, 0.04759896],\n",
       "           ...,\n",
       "           [0.09561975, 0.04759896, 0.04759896, ..., 0.04759896,\n",
       "            0.04759896, 0.04759896],\n",
       "           [0.09561975, 0.04759896, 0.04759896, ..., 0.04759896,\n",
       "            0.04759896, 0.04759896],\n",
       "           [0.09561975, 0.04759896, 0.04759896, ..., 0.04759896,\n",
       "            0.04759896, 0.04759896]],\n",
       "  \n",
       "          [[0.03244692, 0.05092385, 0.05092385, ..., 0.05092385,\n",
       "            0.05092385, 0.05092385],\n",
       "           [0.03229764, 0.0509317 , 0.0509317 , ..., 0.0509317 ,\n",
       "            0.0509317 , 0.0509317 ],\n",
       "           [0.03229764, 0.0509317 , 0.0509317 , ..., 0.0509317 ,\n",
       "            0.0509317 , 0.0509317 ],\n",
       "           ...,\n",
       "           [0.03229764, 0.0509317 , 0.0509317 , ..., 0.0509317 ,\n",
       "            0.0509317 , 0.0509317 ],\n",
       "           [0.03229764, 0.0509317 , 0.0509317 , ..., 0.0509317 ,\n",
       "            0.0509317 , 0.0509317 ],\n",
       "           [0.03229764, 0.0509317 , 0.0509317 , ..., 0.0509317 ,\n",
       "            0.0509317 , 0.0509317 ]]],\n",
       "  \n",
       "  \n",
       "         [[[0.09115691, 0.04783385, 0.04783385, ..., 0.04783385,\n",
       "            0.04783385, 0.04783385],\n",
       "           [0.09561975, 0.04759896, 0.04759896, ..., 0.04759896,\n",
       "            0.04759896, 0.04759896],\n",
       "           [0.09561975, 0.04759896, 0.04759896, ..., 0.04759896,\n",
       "            0.04759896, 0.04759896],\n",
       "           ...,\n",
       "           [0.09561975, 0.04759896, 0.04759896, ..., 0.04759896,\n",
       "            0.04759896, 0.04759896],\n",
       "           [0.09561975, 0.04759896, 0.04759896, ..., 0.04759896,\n",
       "            0.04759896, 0.04759896],\n",
       "           [0.09561975, 0.04759896, 0.04759896, ..., 0.04759896,\n",
       "            0.04759896, 0.04759896]],\n",
       "  \n",
       "          [[0.03244692, 0.05092385, 0.05092385, ..., 0.05092385,\n",
       "            0.05092385, 0.05092385],\n",
       "           [0.03229764, 0.0509317 , 0.0509317 , ..., 0.0509317 ,\n",
       "            0.0509317 , 0.0509317 ],\n",
       "           [0.03229764, 0.0509317 , 0.0509317 , ..., 0.0509317 ,\n",
       "            0.0509317 , 0.0509317 ],\n",
       "           ...,\n",
       "           [0.03229764, 0.0509317 , 0.0509317 , ..., 0.0509317 ,\n",
       "            0.0509317 , 0.0509317 ],\n",
       "           [0.03229764, 0.0509317 , 0.0509317 , ..., 0.0509317 ,\n",
       "            0.0509317 , 0.0509317 ],\n",
       "           [0.03229764, 0.0509317 , 0.0509317 , ..., 0.0509317 ,\n",
       "            0.0509317 , 0.0509317 ]]],\n",
       "  \n",
       "  \n",
       "         [[[0.09115691, 0.04783385, 0.04783385, ..., 0.04783385,\n",
       "            0.04783385, 0.04783385],\n",
       "           [0.09561975, 0.04759896, 0.04759896, ..., 0.04759896,\n",
       "            0.04759896, 0.04759896],\n",
       "           [0.09561975, 0.04759896, 0.04759896, ..., 0.04759896,\n",
       "            0.04759896, 0.04759896],\n",
       "           ...,\n",
       "           [0.09561975, 0.04759896, 0.04759896, ..., 0.04759896,\n",
       "            0.04759896, 0.04759896],\n",
       "           [0.09561975, 0.04759896, 0.04759896, ..., 0.04759896,\n",
       "            0.04759896, 0.04759896],\n",
       "           [0.09561975, 0.04759896, 0.04759896, ..., 0.04759896,\n",
       "            0.04759896, 0.04759896]],\n",
       "  \n",
       "          [[0.03244692, 0.05092385, 0.05092385, ..., 0.05092385,\n",
       "            0.05092385, 0.05092385],\n",
       "           [0.03229764, 0.0509317 , 0.0509317 , ..., 0.0509317 ,\n",
       "            0.0509317 , 0.0509317 ],\n",
       "           [0.03229764, 0.0509317 , 0.0509317 , ..., 0.0509317 ,\n",
       "            0.0509317 , 0.0509317 ],\n",
       "           ...,\n",
       "           [0.03229764, 0.0509317 , 0.0509317 , ..., 0.0509317 ,\n",
       "            0.0509317 , 0.0509317 ],\n",
       "           [0.03229764, 0.0509317 , 0.0509317 , ..., 0.0509317 ,\n",
       "            0.0509317 , 0.0509317 ],\n",
       "           [0.03229764, 0.0509317 , 0.0509317 , ..., 0.0509317 ,\n",
       "            0.0509317 , 0.0509317 ]]],\n",
       "  \n",
       "  \n",
       "         ...,\n",
       "  \n",
       "  \n",
       "         [[[0.09115691, 0.04783385, 0.04783385, ..., 0.04783385,\n",
       "            0.04783385, 0.04783385],\n",
       "           [0.09561975, 0.04759896, 0.04759896, ..., 0.04759896,\n",
       "            0.04759896, 0.04759896],\n",
       "           [0.09561975, 0.04759896, 0.04759896, ..., 0.04759896,\n",
       "            0.04759896, 0.04759896],\n",
       "           ...,\n",
       "           [0.09561975, 0.04759896, 0.04759896, ..., 0.04759896,\n",
       "            0.04759896, 0.04759896],\n",
       "           [0.09561975, 0.04759896, 0.04759896, ..., 0.04759896,\n",
       "            0.04759896, 0.04759896],\n",
       "           [0.09561975, 0.04759896, 0.04759896, ..., 0.04759896,\n",
       "            0.04759896, 0.04759896]],\n",
       "  \n",
       "          [[0.03244692, 0.05092385, 0.05092385, ..., 0.05092385,\n",
       "            0.05092385, 0.05092385],\n",
       "           [0.03229764, 0.0509317 , 0.0509317 , ..., 0.0509317 ,\n",
       "            0.0509317 , 0.0509317 ],\n",
       "           [0.03229764, 0.0509317 , 0.0509317 , ..., 0.0509317 ,\n",
       "            0.0509317 , 0.0509317 ],\n",
       "           ...,\n",
       "           [0.03229764, 0.0509317 , 0.0509317 , ..., 0.0509317 ,\n",
       "            0.0509317 , 0.0509317 ],\n",
       "           [0.03229764, 0.0509317 , 0.0509317 , ..., 0.0509317 ,\n",
       "            0.0509317 , 0.0509317 ],\n",
       "           [0.03229764, 0.0509317 , 0.0509317 , ..., 0.0509317 ,\n",
       "            0.0509317 , 0.0509317 ]]],\n",
       "  \n",
       "  \n",
       "         [[[0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           ...,\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ]],\n",
       "  \n",
       "          [[0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           ...,\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ]]],\n",
       "  \n",
       "  \n",
       "         [[[0.09115691, 0.04783385, 0.04783385, ..., 0.04783385,\n",
       "            0.04783385, 0.04783385],\n",
       "           [0.09561975, 0.04759896, 0.04759896, ..., 0.04759896,\n",
       "            0.04759896, 0.04759896],\n",
       "           [0.09561975, 0.04759896, 0.04759896, ..., 0.04759896,\n",
       "            0.04759896, 0.04759896],\n",
       "           ...,\n",
       "           [0.09561975, 0.04759896, 0.04759896, ..., 0.04759896,\n",
       "            0.04759896, 0.04759896],\n",
       "           [0.09561975, 0.04759896, 0.04759896, ..., 0.04759896,\n",
       "            0.04759896, 0.04759896],\n",
       "           [0.09561975, 0.04759896, 0.04759896, ..., 0.04759896,\n",
       "            0.04759896, 0.04759896]],\n",
       "  \n",
       "          [[0.03244692, 0.05092385, 0.05092385, ..., 0.05092385,\n",
       "            0.05092385, 0.05092385],\n",
       "           [0.03229764, 0.0509317 , 0.0509317 , ..., 0.0509317 ,\n",
       "            0.0509317 , 0.0509317 ],\n",
       "           [0.03229764, 0.0509317 , 0.0509317 , ..., 0.0509317 ,\n",
       "            0.0509317 , 0.0509317 ],\n",
       "           ...,\n",
       "           [0.03229764, 0.0509317 , 0.0509317 , ..., 0.0509317 ,\n",
       "            0.0509317 , 0.0509317 ],\n",
       "           [0.03229764, 0.0509317 , 0.0509317 , ..., 0.0509317 ,\n",
       "            0.0509317 , 0.0509317 ],\n",
       "           [0.03229764, 0.0509317 , 0.0509317 , ..., 0.0509317 ,\n",
       "            0.0509317 , 0.0509317 ]]]], dtype=float32)>,\n",
       "  'decoder_layer4_block1': <tf.Tensor: shape=(1000, 2, 20, 20), dtype=float32, numpy=\n",
       "  array([[[[0.05181941, 0.04990425, 0.04990425, ..., 0.04990425,\n",
       "            0.04990425, 0.04990425],\n",
       "           [0.05205255, 0.04989197, 0.04989197, ..., 0.04989197,\n",
       "            0.04989197, 0.04989197],\n",
       "           [0.05205255, 0.04989197, 0.04989197, ..., 0.04989197,\n",
       "            0.04989197, 0.04989197],\n",
       "           ...,\n",
       "           [0.05205255, 0.04989197, 0.04989197, ..., 0.04989197,\n",
       "            0.04989197, 0.04989197],\n",
       "           [0.05205255, 0.04989197, 0.04989197, ..., 0.04989197,\n",
       "            0.04989197, 0.04989197],\n",
       "           [0.05205255, 0.04989197, 0.04989197, ..., 0.04989197,\n",
       "            0.04989197, 0.04989197]],\n",
       "  \n",
       "          [[0.04727025, 0.05014367, 0.05014367, ..., 0.05014367,\n",
       "            0.05014367, 0.05014367],\n",
       "           [0.04772177, 0.05011991, 0.05011991, ..., 0.05011991,\n",
       "            0.05011991, 0.05011991],\n",
       "           [0.04772177, 0.05011991, 0.05011991, ..., 0.05011991,\n",
       "            0.05011991, 0.05011991],\n",
       "           ...,\n",
       "           [0.04772177, 0.05011991, 0.05011991, ..., 0.05011991,\n",
       "            0.05011991, 0.05011991],\n",
       "           [0.04772177, 0.05011991, 0.05011991, ..., 0.05011991,\n",
       "            0.05011991, 0.05011991],\n",
       "           [0.04772177, 0.05011991, 0.05011991, ..., 0.05011991,\n",
       "            0.05011991, 0.05011991]]],\n",
       "  \n",
       "  \n",
       "         [[[0.05181941, 0.04990425, 0.04990425, ..., 0.04990425,\n",
       "            0.04990425, 0.04990425],\n",
       "           [0.05205255, 0.04989197, 0.04989197, ..., 0.04989197,\n",
       "            0.04989197, 0.04989197],\n",
       "           [0.05205255, 0.04989197, 0.04989197, ..., 0.04989197,\n",
       "            0.04989197, 0.04989197],\n",
       "           ...,\n",
       "           [0.05205255, 0.04989197, 0.04989197, ..., 0.04989197,\n",
       "            0.04989197, 0.04989197],\n",
       "           [0.05205255, 0.04989197, 0.04989197, ..., 0.04989197,\n",
       "            0.04989197, 0.04989197],\n",
       "           [0.05205255, 0.04989197, 0.04989197, ..., 0.04989197,\n",
       "            0.04989197, 0.04989197]],\n",
       "  \n",
       "          [[0.04727025, 0.05014367, 0.05014367, ..., 0.05014367,\n",
       "            0.05014367, 0.05014367],\n",
       "           [0.04772177, 0.05011991, 0.05011991, ..., 0.05011991,\n",
       "            0.05011991, 0.05011991],\n",
       "           [0.04772177, 0.05011991, 0.05011991, ..., 0.05011991,\n",
       "            0.05011991, 0.05011991],\n",
       "           ...,\n",
       "           [0.04772177, 0.05011991, 0.05011991, ..., 0.05011991,\n",
       "            0.05011991, 0.05011991],\n",
       "           [0.04772177, 0.05011991, 0.05011991, ..., 0.05011991,\n",
       "            0.05011991, 0.05011991],\n",
       "           [0.04772177, 0.05011991, 0.05011991, ..., 0.05011991,\n",
       "            0.05011991, 0.05011991]]],\n",
       "  \n",
       "  \n",
       "         [[[0.05181941, 0.04990425, 0.04990425, ..., 0.04990425,\n",
       "            0.04990425, 0.04990425],\n",
       "           [0.05205255, 0.04989197, 0.04989197, ..., 0.04989197,\n",
       "            0.04989197, 0.04989197],\n",
       "           [0.05205255, 0.04989197, 0.04989197, ..., 0.04989197,\n",
       "            0.04989197, 0.04989197],\n",
       "           ...,\n",
       "           [0.05205255, 0.04989197, 0.04989197, ..., 0.04989197,\n",
       "            0.04989197, 0.04989197],\n",
       "           [0.05205255, 0.04989197, 0.04989197, ..., 0.04989197,\n",
       "            0.04989197, 0.04989197],\n",
       "           [0.05205255, 0.04989197, 0.04989197, ..., 0.04989197,\n",
       "            0.04989197, 0.04989197]],\n",
       "  \n",
       "          [[0.04727025, 0.05014367, 0.05014367, ..., 0.05014367,\n",
       "            0.05014367, 0.05014367],\n",
       "           [0.04772177, 0.05011991, 0.05011991, ..., 0.05011991,\n",
       "            0.05011991, 0.05011991],\n",
       "           [0.04772177, 0.05011991, 0.05011991, ..., 0.05011991,\n",
       "            0.05011991, 0.05011991],\n",
       "           ...,\n",
       "           [0.04772177, 0.05011991, 0.05011991, ..., 0.05011991,\n",
       "            0.05011991, 0.05011991],\n",
       "           [0.04772177, 0.05011991, 0.05011991, ..., 0.05011991,\n",
       "            0.05011991, 0.05011991],\n",
       "           [0.04772177, 0.05011991, 0.05011991, ..., 0.05011991,\n",
       "            0.05011991, 0.05011991]]],\n",
       "  \n",
       "  \n",
       "         ...,\n",
       "  \n",
       "  \n",
       "         [[[0.05181941, 0.04990425, 0.04990425, ..., 0.04990425,\n",
       "            0.04990425, 0.04990425],\n",
       "           [0.05205255, 0.04989197, 0.04989197, ..., 0.04989197,\n",
       "            0.04989197, 0.04989197],\n",
       "           [0.05205255, 0.04989197, 0.04989197, ..., 0.04989197,\n",
       "            0.04989197, 0.04989197],\n",
       "           ...,\n",
       "           [0.05205255, 0.04989197, 0.04989197, ..., 0.04989197,\n",
       "            0.04989197, 0.04989197],\n",
       "           [0.05205255, 0.04989197, 0.04989197, ..., 0.04989197,\n",
       "            0.04989197, 0.04989197],\n",
       "           [0.05205255, 0.04989197, 0.04989197, ..., 0.04989197,\n",
       "            0.04989197, 0.04989197]],\n",
       "  \n",
       "          [[0.04727025, 0.05014367, 0.05014367, ..., 0.05014367,\n",
       "            0.05014367, 0.05014367],\n",
       "           [0.04772177, 0.05011991, 0.05011991, ..., 0.05011991,\n",
       "            0.05011991, 0.05011991],\n",
       "           [0.04772177, 0.05011991, 0.05011991, ..., 0.05011991,\n",
       "            0.05011991, 0.05011991],\n",
       "           ...,\n",
       "           [0.04772177, 0.05011991, 0.05011991, ..., 0.05011991,\n",
       "            0.05011991, 0.05011991],\n",
       "           [0.04772177, 0.05011991, 0.05011991, ..., 0.05011991,\n",
       "            0.05011991, 0.05011991],\n",
       "           [0.04772177, 0.05011991, 0.05011991, ..., 0.05011991,\n",
       "            0.05011991, 0.05011991]]],\n",
       "  \n",
       "  \n",
       "         [[[0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           ...,\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ]],\n",
       "  \n",
       "          [[0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           ...,\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ]]],\n",
       "  \n",
       "  \n",
       "         [[[0.05181941, 0.04990425, 0.04990425, ..., 0.04990425,\n",
       "            0.04990425, 0.04990425],\n",
       "           [0.05205255, 0.04989197, 0.04989197, ..., 0.04989197,\n",
       "            0.04989197, 0.04989197],\n",
       "           [0.05205255, 0.04989197, 0.04989197, ..., 0.04989197,\n",
       "            0.04989197, 0.04989197],\n",
       "           ...,\n",
       "           [0.05205255, 0.04989197, 0.04989197, ..., 0.04989197,\n",
       "            0.04989197, 0.04989197],\n",
       "           [0.05205255, 0.04989197, 0.04989197, ..., 0.04989197,\n",
       "            0.04989197, 0.04989197],\n",
       "           [0.05205255, 0.04989197, 0.04989197, ..., 0.04989197,\n",
       "            0.04989197, 0.04989197]],\n",
       "  \n",
       "          [[0.04727025, 0.05014367, 0.05014367, ..., 0.05014367,\n",
       "            0.05014367, 0.05014367],\n",
       "           [0.04772177, 0.05011991, 0.05011991, ..., 0.05011991,\n",
       "            0.05011991, 0.05011991],\n",
       "           [0.04772177, 0.05011991, 0.05011991, ..., 0.05011991,\n",
       "            0.05011991, 0.05011991],\n",
       "           ...,\n",
       "           [0.04772177, 0.05011991, 0.05011991, ..., 0.05011991,\n",
       "            0.05011991, 0.05011991],\n",
       "           [0.04772177, 0.05011991, 0.05011991, ..., 0.05011991,\n",
       "            0.05011991, 0.05011991],\n",
       "           [0.04772177, 0.05011991, 0.05011991, ..., 0.05011991,\n",
       "            0.05011991, 0.05011991]]]], dtype=float32)>,\n",
       "  'decoder_layer4_block2': <tf.Tensor: shape=(1000, 2, 20, 20), dtype=float32, numpy=\n",
       "  array([[[[0.09692972, 0.04753001, 0.04753001, ..., 0.04753001,\n",
       "            0.04753001, 0.04753001],\n",
       "           [0.10415106, 0.04714994, 0.04714994, ..., 0.04714994,\n",
       "            0.04714994, 0.04714994],\n",
       "           [0.10415106, 0.04714994, 0.04714994, ..., 0.04714994,\n",
       "            0.04714994, 0.04714994],\n",
       "           ...,\n",
       "           [0.10415106, 0.04714994, 0.04714994, ..., 0.04714994,\n",
       "            0.04714994, 0.04714994],\n",
       "           [0.10415106, 0.04714994, 0.04714994, ..., 0.04714994,\n",
       "            0.04714994, 0.04714994],\n",
       "           [0.10415106, 0.04714994, 0.04714994, ..., 0.04714994,\n",
       "            0.04714994, 0.04714994]],\n",
       "  \n",
       "          [[0.02590569, 0.05126812, 0.05126812, ..., 0.05126812,\n",
       "            0.05126812, 0.05126812],\n",
       "           [0.02553811, 0.05128747, 0.05128747, ..., 0.05128747,\n",
       "            0.05128747, 0.05128747],\n",
       "           [0.02553811, 0.05128747, 0.05128747, ..., 0.05128747,\n",
       "            0.05128747, 0.05128747],\n",
       "           ...,\n",
       "           [0.02553811, 0.05128747, 0.05128747, ..., 0.05128747,\n",
       "            0.05128747, 0.05128747],\n",
       "           [0.02553811, 0.05128747, 0.05128747, ..., 0.05128747,\n",
       "            0.05128747, 0.05128747],\n",
       "           [0.02553811, 0.05128747, 0.05128747, ..., 0.05128747,\n",
       "            0.05128747, 0.05128747]]],\n",
       "  \n",
       "  \n",
       "         [[[0.09692972, 0.04753001, 0.04753001, ..., 0.04753001,\n",
       "            0.04753001, 0.04753001],\n",
       "           [0.10415106, 0.04714994, 0.04714994, ..., 0.04714994,\n",
       "            0.04714994, 0.04714994],\n",
       "           [0.10415106, 0.04714994, 0.04714994, ..., 0.04714994,\n",
       "            0.04714994, 0.04714994],\n",
       "           ...,\n",
       "           [0.10415106, 0.04714994, 0.04714994, ..., 0.04714994,\n",
       "            0.04714994, 0.04714994],\n",
       "           [0.10415106, 0.04714994, 0.04714994, ..., 0.04714994,\n",
       "            0.04714994, 0.04714994],\n",
       "           [0.10415106, 0.04714994, 0.04714994, ..., 0.04714994,\n",
       "            0.04714994, 0.04714994]],\n",
       "  \n",
       "          [[0.02590569, 0.05126812, 0.05126812, ..., 0.05126812,\n",
       "            0.05126812, 0.05126812],\n",
       "           [0.02553811, 0.05128747, 0.05128747, ..., 0.05128747,\n",
       "            0.05128747, 0.05128747],\n",
       "           [0.02553811, 0.05128747, 0.05128747, ..., 0.05128747,\n",
       "            0.05128747, 0.05128747],\n",
       "           ...,\n",
       "           [0.02553811, 0.05128747, 0.05128747, ..., 0.05128747,\n",
       "            0.05128747, 0.05128747],\n",
       "           [0.02553811, 0.05128747, 0.05128747, ..., 0.05128747,\n",
       "            0.05128747, 0.05128747],\n",
       "           [0.02553811, 0.05128747, 0.05128747, ..., 0.05128747,\n",
       "            0.05128747, 0.05128747]]],\n",
       "  \n",
       "  \n",
       "         [[[0.09692972, 0.04753001, 0.04753001, ..., 0.04753001,\n",
       "            0.04753001, 0.04753001],\n",
       "           [0.10415106, 0.04714994, 0.04714994, ..., 0.04714994,\n",
       "            0.04714994, 0.04714994],\n",
       "           [0.10415106, 0.04714994, 0.04714994, ..., 0.04714994,\n",
       "            0.04714994, 0.04714994],\n",
       "           ...,\n",
       "           [0.10415106, 0.04714994, 0.04714994, ..., 0.04714994,\n",
       "            0.04714994, 0.04714994],\n",
       "           [0.10415106, 0.04714994, 0.04714994, ..., 0.04714994,\n",
       "            0.04714994, 0.04714994],\n",
       "           [0.10415106, 0.04714994, 0.04714994, ..., 0.04714994,\n",
       "            0.04714994, 0.04714994]],\n",
       "  \n",
       "          [[0.02590569, 0.05126812, 0.05126812, ..., 0.05126812,\n",
       "            0.05126812, 0.05126812],\n",
       "           [0.02553811, 0.05128747, 0.05128747, ..., 0.05128747,\n",
       "            0.05128747, 0.05128747],\n",
       "           [0.02553811, 0.05128747, 0.05128747, ..., 0.05128747,\n",
       "            0.05128747, 0.05128747],\n",
       "           ...,\n",
       "           [0.02553811, 0.05128747, 0.05128747, ..., 0.05128747,\n",
       "            0.05128747, 0.05128747],\n",
       "           [0.02553811, 0.05128747, 0.05128747, ..., 0.05128747,\n",
       "            0.05128747, 0.05128747],\n",
       "           [0.02553811, 0.05128747, 0.05128747, ..., 0.05128747,\n",
       "            0.05128747, 0.05128747]]],\n",
       "  \n",
       "  \n",
       "         ...,\n",
       "  \n",
       "  \n",
       "         [[[0.09692972, 0.04753001, 0.04753001, ..., 0.04753001,\n",
       "            0.04753001, 0.04753001],\n",
       "           [0.10415106, 0.04714994, 0.04714994, ..., 0.04714994,\n",
       "            0.04714994, 0.04714994],\n",
       "           [0.10415106, 0.04714994, 0.04714994, ..., 0.04714994,\n",
       "            0.04714994, 0.04714994],\n",
       "           ...,\n",
       "           [0.10415106, 0.04714994, 0.04714994, ..., 0.04714994,\n",
       "            0.04714994, 0.04714994],\n",
       "           [0.10415106, 0.04714994, 0.04714994, ..., 0.04714994,\n",
       "            0.04714994, 0.04714994],\n",
       "           [0.10415106, 0.04714994, 0.04714994, ..., 0.04714994,\n",
       "            0.04714994, 0.04714994]],\n",
       "  \n",
       "          [[0.02590569, 0.05126812, 0.05126812, ..., 0.05126812,\n",
       "            0.05126812, 0.05126812],\n",
       "           [0.02553811, 0.05128747, 0.05128747, ..., 0.05128747,\n",
       "            0.05128747, 0.05128747],\n",
       "           [0.02553811, 0.05128747, 0.05128747, ..., 0.05128747,\n",
       "            0.05128747, 0.05128747],\n",
       "           ...,\n",
       "           [0.02553811, 0.05128747, 0.05128747, ..., 0.05128747,\n",
       "            0.05128747, 0.05128747],\n",
       "           [0.02553811, 0.05128747, 0.05128747, ..., 0.05128747,\n",
       "            0.05128747, 0.05128747],\n",
       "           [0.02553811, 0.05128747, 0.05128747, ..., 0.05128747,\n",
       "            0.05128747, 0.05128747]]],\n",
       "  \n",
       "  \n",
       "         [[[0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           ...,\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ]],\n",
       "  \n",
       "          [[0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           ...,\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ],\n",
       "           [0.05      , 0.05      , 0.05      , ..., 0.05      ,\n",
       "            0.05      , 0.05      ]]],\n",
       "  \n",
       "  \n",
       "         [[[0.09692972, 0.04753001, 0.04753001, ..., 0.04753001,\n",
       "            0.04753001, 0.04753001],\n",
       "           [0.10415106, 0.04714994, 0.04714994, ..., 0.04714994,\n",
       "            0.04714994, 0.04714994],\n",
       "           [0.10415106, 0.04714994, 0.04714994, ..., 0.04714994,\n",
       "            0.04714994, 0.04714994],\n",
       "           ...,\n",
       "           [0.10415106, 0.04714994, 0.04714994, ..., 0.04714994,\n",
       "            0.04714994, 0.04714994],\n",
       "           [0.10415106, 0.04714994, 0.04714994, ..., 0.04714994,\n",
       "            0.04714994, 0.04714994],\n",
       "           [0.10415106, 0.04714994, 0.04714994, ..., 0.04714994,\n",
       "            0.04714994, 0.04714994]],\n",
       "  \n",
       "          [[0.02590569, 0.05126812, 0.05126812, ..., 0.05126812,\n",
       "            0.05126812, 0.05126812],\n",
       "           [0.02553811, 0.05128747, 0.05128747, ..., 0.05128747,\n",
       "            0.05128747, 0.05128747],\n",
       "           [0.02553811, 0.05128747, 0.05128747, ..., 0.05128747,\n",
       "            0.05128747, 0.05128747],\n",
       "           ...,\n",
       "           [0.02553811, 0.05128747, 0.05128747, ..., 0.05128747,\n",
       "            0.05128747, 0.05128747],\n",
       "           [0.02553811, 0.05128747, 0.05128747, ..., 0.05128747,\n",
       "            0.05128747, 0.05128747],\n",
       "           [0.02553811, 0.05128747, 0.05128747, ..., 0.05128747,\n",
       "            0.05128747, 0.05128747]]]], dtype=float32)>})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(x,y,  training=False, \n",
    "    enc_padding_mask=None, \n",
    "    look_ahead_mask=None,\n",
    "    dec_padding_mask=None) #y,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "You must compile your model before training/testing. Use `model.compile(optimizer, loss)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-015c15ed5e77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/anaconda/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    786\u001b[0m     \u001b[0;31m# Legacy graph support is contained in `training_v1.Model`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0mversion_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisallow_legacy_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_compile_was_called\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    789\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_call_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fit'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_assert_compile_was_called\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1633\u001b[0m     \u001b[0;31m# (i.e. whether the model is built and its inputs/outputs are set).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1634\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1635\u001b[0;31m       raise RuntimeError('You must compile your model before '\n\u001b[0m\u001b[1;32m   1636\u001b[0m                          \u001b[0;34m'training/testing. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1637\u001b[0m                          'Use `model.compile(optimizer, loss)`.')\n",
      "\u001b[0;31mRuntimeError\u001b[0m: You must compile your model before training/testing. Use `model.compile(optimizer, loss)`."
     ]
    }
   ],
   "source": [
    "net.fit(x,y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
